# -*- coding: utf-8 -*-
"""best_threshold.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u_CJ5NAh96kPtdnZUDnj5J7FVY4RZy2u
"""

import subprocess
import sys

# Function to upgrade pandas
def install_or_upgrade_pandas():
    try:
        # Check if pandas is already installed
        import pandas as pd
        print(f"Pandas version: {pd.__version__}")
    except ImportError:
        # If pandas is not installed, install it
        print("Pandas not found, installing it...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "pandas"])
    else:
        # If pandas is installed, upgrade it
        print("Upgrading pandas to the latest version...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "pandas"])

# Call the function to install or upgrade pandas
install_or_upgrade_pandas()

# Now you can import pandas
import pandas as pd

import pandas as pd
import numpy as np
from scipy.spatial.distance import euclidean
from scipy.stats import median_abs_deviation  # Import for MAD calculation
from google.colab import drive
import sys

# Mount the drive
drive.mount('/content/drive', force_remount=True)
sys.path.append('/content/drive/MyDrive/Colab Notebooks/standardization')
from encoder import encode_categorical
# Function to calculate the best threshold for random effect analysis
def calculate_best_threshold(df, columns):
    thresholds = {}

    # Clean the data by dropping rows with NaN values and Inf values
    df = df.copy()
    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=columns)

    # Ensure numeric columns only
    numeric_columns = [col for col in columns if pd.api.types.is_numeric_dtype(df[col])]
    df = df[numeric_columns]

    # Remove columns with insufficient data
    df = df.dropna(axis=1, how='any')

    # Calculating different thresholds
    def iqr_method(df, columns):
        iqr_values = []
        for col in columns:
            iqr = df[col].quantile(0.75) - df[col].quantile(0.25)
            if np.isfinite(iqr):
                iqr_values.append(iqr)
        return sum(iqr_values) / len(iqr_values) if iqr_values else 1e-3

    def std_method(df, columns):
        std_values = []
        for col in columns:
            std = df[col].std()
            if np.isfinite(std):
                std_values.append(std)
        return sum(std_values) / len(std_values) if std_values else 1e-3

    def elbow_method(df, columns):
        std_values = []
        for col in columns:
            std = df[col].std()
            if np.isfinite(std):
                std_values.append(std)
        if len(std_values) < 2:
            return 1e-3  # Not enough data for elbow method

        sorted_values = sorted(std_values)
        distances = []
        for i in range(1, len(sorted_values)):
            if np.isfinite(sorted_values[i]) and np.isfinite(sorted_values[0]):
                point1 = np.array([i, sorted_values[i]])
                point2 = np.array([0, sorted_values[0]])
                distance = euclidean(point1, point2)
                distances.append(distance)
            else:
                distances.append(0)
        elbow_index = np.argmax(distances) if len(distances) > 0 else 0
        return sorted_values[elbow_index] if sorted_values else 1e-3

    def variance_ratio_method(df, columns):
        variances = []
        for col in columns:
            var = df[col].var()
            if np.isfinite(var):
                variances.append(var)
        if len(variances) < 2 or min(variances) <= 0:
            return 1e-3  # Not enough data or zero variance
        return max(variances) / min(variances)

    def mad_method(df, columns):
        mad_values = []
        for col in columns:
            mad = median_abs_deviation(df[col], scale='normal')
            if np.isfinite(mad):
                mad_values.append(mad)
        return sum(mad_values) / len(mad_values) if mad_values else 1e-3

    # Store thresholds from each method
    thresholds['IQR'] = iqr_method(df, df.columns)
    thresholds['Standard Deviation'] = std_method(df, df.columns)
    thresholds['Elbow'] = elbow_method(df, df.columns)
    thresholds['Variance Ratio'] = variance_ratio_method(df, df.columns)
    thresholds['MAD'] = mad_method(df, df.columns)

    # Dynamically choose the best threshold method based on data characteristics
    def choose_best_method(df, columns):
        # Example criterion: If any column has a variance above a certain threshold, prefer Standard Deviation
        high_variance_cols = [col for col in columns if df[col].var() > 2.0]  # Adjust threshold as needed

        if len(high_variance_cols) > 0:
            return 'Standard Deviation'

        # If variance is small but there's a large difference between quartiles, use IQR
        large_iqr_cols = [col for col in columns if (df[col].quantile(0.75) - df[col].quantile(0.25)) > 1.5]
        if len(large_iqr_cols) > 0:
            return 'IQR'

        # If there are significant outliers, prefer MAD
        large_mad_cols = [col for col in columns if median_abs_deviation(df[col], scale='normal') > 1.0]
        if len(large_mad_cols) > 0:
            return 'MAD'

        # If data shows a large range in variance, use Variance Ratio
        if thresholds['Variance Ratio'] > 5.0:
            return 'Variance Ratio'

        # Fall back on Elbow method if none of the above are strongly indicated
        return 'Elbow'

    best_method = choose_best_method(df, numeric_columns)
    print(f"Selected best method: {best_method}")

    return thresholds[best_method]

# Load the data and encode it (assuming encode_categorical is already defined)
data = pd.read_csv("/content/drive/MyDrive/test_data.csv")
data = pd.DataFrame(data)
encoded_df = encode_categorical(data)

# Ensure there are no NaN or Inf values in the encoded DataFrame
encoded_df = encoded_df.replace([np.inf, -np.inf], np.nan)
numeric_columns = encoded_df.select_dtypes(include=[np.number]).columns
encoded_df = encoded_df.dropna(subset=numeric_columns)

# Calculate the best threshold based on dynamic method selection
best_threshold = calculate_best_threshold(encoded_df, numeric_columns.tolist())

# Print the selected best threshold value
print(f"Best threshold value: {best_threshold}")